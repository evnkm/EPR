# Pareto-Principled ML Research Program: Compositional EBMs for ARC-AGI

**Your core bet is well-positioned but needs strategic scoping.** The bridge between Yilun Du's compositional energy-based models and discrete symbolic reasoning represents a genuine research gap. GFlowNets have already been applied to ARC (TMLR 2025), masked diffusion achieved 2nd place in ARC Prize 2025, and IRED demonstrates EBM+diffusion works for algorithmic reasoning—yet nobody has combined compositional energy functions with discrete structure sampling for ARC. The technical path exists; execution risk is moderate.

Following Schulman's principle that "your goal will give you a perspective differentiated from the rest of the community," this direction offers high leverage: it's not the same literature everyone else is reading (TTT, LLM prompting) but builds directly on your advisor's work with clear generalization potential beyond ARC.

## Schulman's framework applied to your 2-week sprint

John Schulman's core thesis—"the keys to success are working on the right problems, making continual progress, and achieving continual personal growth"—maps directly onto your situation. His most critical insight for you: **goal-driven research beats idea-driven research** because it leads you to "questions no one else is asking." Your goal (adapting EBMs to discrete symbolic reasoning) is inherently differentiated from the TTT-dominated ARC literature.

Three red flags to monitor from Schulman's guide:

1. **Half-finished projects pattern**: After 2 weeks, you must have a deliverable (paper or codebase), not abandoned experiments
2. **Domain-specific hacks**: Solutions must generalize beyond ARC to program synthesis, planning, or other discrete reasoning domains
3. **Novelty temptation**: Don't abandon your approach for "newer ideas" mid-sprint; the grass always looks greener

His tactical advice: keep a daily notebook with experiments/insights, conduct reviews on Day 7 and Day 14, and maintain a **10:1 complexity rule**—a 10% improvement should require only ~2 lines of code change; anything more complex needs proportionally larger gains.

## Three subproblems ranked by leverage-to-risk ratio

### Subproblem 1: Compositional energy functions for ARC transformation primitives

**Leverage/Risk: HIGH / MEDIUM** — This is your 80/20 play.

The core technical question: can you define energy functions E(output|transformation, input) for discrete ARC transformations such that summing energies (Du's product-of-experts) captures transformation compositions? This directly extends Du's compositional paradigm to discrete structures without requiring full end-to-end learning.

**Scope**: Implement **5-7 hand-defined energy functions** for core ARC primitives (rotation, reflection, color mapping, crop/tile, translation). Energy should be low when the transformation correctly relates input→output, high otherwise. Test whether energy composition (E_total = ΣE_i) correctly identifies multi-step transformation sequences on simple ARC tasks.

**Technical approach**: Define E(output|T, input) = ||T(input) - output||² + regularization for interpretable transformations T. For compositional transformations, sum energies: E(output|T₁∘T₂, input) = E(T₁(input)|T₂, output). Sample transformation sequences using Gibbs-with-Gradients (GWG) over the discrete transformation space, not Langevin dynamics.

**Minimum viable result**: Demonstrate on **10-20 curated ARC tasks** that (a) individual energy functions correctly score transformation quality, and (b) composed energies identify correct 2-step transformation sequences with >70% accuracy. This proves the representation works.

**Stretch goal**: Show zero-shot composition—train energy functions on single transformations, test on compositions never seen during training. If this works, you have a strong NeurIPS paper.

**Estimated time**: 7-10 days for core implementation and experiments.

**Decision point (Day 5)**: If energy functions don't discriminate correct from incorrect transformations on single-step tasks, pivot to Subproblem 2. If they do discriminate but composition fails, debug the composition mechanism for 2 more days before pivoting.

---

### Subproblem 2: GFlowNet sampling from compositional energy landscapes

**Leverage/Risk: MEDIUM / MEDIUM** — Strong backup with existing validation.

GFlowNets have been directly applied to ARC (Hwang et al., TMLR 2025), proving the sampling mechanism works for discrete grid transformations. Your contribution: replace their reward function with **factored compositional energy** inspired by Du's work, enabling more structured generalization.

**Scope**: Fork the existing GFN_to_ARC codebase (github.com/GIST-DSLab/GFN_to_ARC). Replace their reward with R(s) = exp(-E_compositional(s)) where E_compositional sums learned or hand-defined energy components. Evaluate whether compositional energy guidance improves sample diversity and solution quality compared to baseline GFlowNet.

**Technical approach**: Define reward as R(output, target) = exp(-(E_shape + E_color + E_structure)) where each component scores a different aspect of output-target alignment. Train GFlowNet to sample transformation sequences proportional to this compositional reward. Compare diversity metrics (number of unique valid solutions) and accuracy against baseline.

**Minimum viable result**: Show that compositional energy factorization improves either (a) sample diversity (more distinct valid solutions) or (b) accuracy (higher solve rate) on **50+ ARC tasks** compared to monolithic reward baseline.

**Stretch goal**: Learn the energy decomposition unsupervised (COMET-style) rather than hand-defining it. This would be a significant contribution to the GFlowNet literature.

**Estimated time**: 5-7 days (leveraging existing codebase).

**Decision point (Day 3)**: If GFlowNet training is unstable or you can't reproduce baseline results, pivot to Subproblem 3 (pure analysis/position paper).

---

### Subproblem 3: Theoretical analysis connecting compositional EBMs, GFlowNets, and discrete structure generation

**Leverage/Risk: LOW / LOW** — Safe fallback with guaranteed output.

This is your insurance policy. If implementations fail, a well-written position paper or analysis piece connecting Du's compositional framework to discrete domains (via GFlowNets, discrete diffusion, or Gibbs sampling) would establish your thinking, cite the right literature, and position you for future work.

**Scope**: Write a 4-6 page workshop paper analyzing: (1) mathematical correspondence between compositional energy functions and GFlowNet rewards, (2) conditions under which energy composition preserves sampling guarantees, (3) concrete technical roadmap for ARC application with complexity analysis.

**Technical approach**: Prove that if E_1, E_2 satisfy certain conditions, then R = exp(-(E_1 + E_2)) gives a valid GFlowNet reward with predictable mode structure. Analyze computational complexity of exact vs. approximate inference. Include small-scale experiments validating key claims.

**Minimum viable result**: 4-page workshop paper with clear theoretical contribution and preliminary experiments on toy discrete domains (not full ARC).

**Stretch goal**: Include ablation experiments showing when composition helps vs. hurts, with theoretical explanation.

**Estimated time**: 5-7 days for writing + small experiments.

**Decision point**: This is the fallback—no pivot needed. Execute if Subproblems 1-2 fail by Day 8.

## Decision tree for the 2-week sprint

```
Day 1-2: Setup + Subproblem 1 implementation begins
    |
Day 5 checkpoint: Do single-transformation energies discriminate correctly?
    |
    ├─ YES → Continue Subproblem 1 through Day 10
    |         Day 10: Do compositions work?
    |         ├─ YES → Write up results, target ICLR workshops
    |         └─ NO → Add Subproblem 2 as augmentation, write combined paper
    |
    └─ NO → Pivot to Subproblem 2 (GFlowNet fork)
              Day 8: Can you reproduce baseline + improve with factored reward?
              ├─ YES → Complete experiments, write workshop paper
              └─ NO → Execute Subproblem 3 (analysis paper), guaranteed output

```

## Publication venues mapped to realistic timelines

| Target | Deadline | Feasibility | Scope Required |
| --- | --- | --- | --- |
| **ArXiv preprint** | Anytime | ✅ Highly achievable | Any stage—establishes priority immediately |
| **ICLR 2026 Workshop** | ~Jan 30, 2026 | ⚠️ Tight but possible | 4-6 pages, preliminary results OK |
| **ICML 2026 Workshop** | Apr 24, 2026 | ✅ Comfortable | 4-6 pages, more complete experiments |
| **NeurIPS 2026** | May 15, 2026 | ✅ Full development time | 8+ pages, comprehensive evaluation |
| **TMLR** | Rolling | ✅ Anytime | Full paper, ~3 month review |

**Recommended pathway**: Post to arXiv within 2 weeks (establishes priority), submit to ICLR 2026 workshop by Jan 30 if results look promising, then continue developing toward NeurIPS 2026 (May 15 deadline gives you 4 months for full experiments).

For workshops, target: **Compositional Learning workshops**, **Neurosymbolic AI workshops**, **Reasoning in Language Models workshops**, or **Programmatic Agents workshops** at ICML/NeurIPS 2026.

## Prioritized reading list: 10 essential papers

**Foundational (read in first 2 days):**

1. **Schulman, "An Opinionated Guide to ML Research"** (joschu.net) — Your process framework
2. **Du et al., "Compositional Visual Generation with Energy Based Models"** (NeurIPS 2020) — Core compositional EBM method
3. **Bengio et al., "GFlowNet Foundations"** (JMLR 2023) — GFlowNet theory and EBM connection

**Technical bridges (read Days 3-5):**

1. **Hwang et al., "Solution Augmentation for ARC Using GFlowNet"** (TMLR 2025) — Direct GFlowNet-ARC application, fork this codebase
2. **Grathwohl et al., "Gibbs-with-Gradients: Scalable Sampling for Discrete Distributions"** (ICML 2021) — Discrete EBM sampling
3. **Du et al., "Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC"** (ICML 2023) — Why samplers matter for composition

**ARC-specific (skim for context):**

1. **~~ARChitects 2025 solution writeup** — Masked diffusion on ARC, 2nd place approach~~
2. **MindsAI technical report** (if available) — TTT baseline to understand
3. **Jolicoeur-Martineau, "Tiny Recursive Model"** — 7M param model achieving 45% on ARC-1

**Deeper theory (read if time permits):**

1. **Hu et al., "GFlowNet-EM for Learning Compositional Latent Variable Models"** (ICML 2023) — GFlowNets for compositional discrete latents

## Risk assessment framework

**What makes ARC research risky:**

- **Benchmark saturation**: If someone publishes similar approach before you, novelty diminishes
- **Negative results**: Energy composition might not help for discrete domains (possible but testable early)
- **Implementation complexity**: GFlowNets + energy functions + ARC environment is significant engineering
- **Evaluation overhead**: ARC evaluation requires running many tasks; debugging cycles are slow

**What makes your specific direction safer:**

- **Clear technical gap**: Nobody has combined Du's compositional EBMs with GFlowNets for ARC
- **Existing codebase**: GFN_to_ARC exists—you're not starting from zero
- **Multiple fallback positions**: Theory paper, ablation study, or negative result paper all publishable
- **Advisor alignment**: Direct extension of Du's work; he can provide quick feedback on technical direction

**Subproblems by risk profile:**

- Subproblem 1 (Compositional energies): **Medium risk, high reward** — Novel but testable early
- Subproblem 2 (GFlowNet integration): **Low-medium risk, medium reward** — Builds on proven approach
- Subproblem 3 (Analysis paper): **Low risk, low-medium reward** — Guaranteed output, less impressive

## Concrete 2-week schedule

**Week 1: Implementation Sprint**

- ~~Day 1: Read Schulman + Du compositional paper, set up environment, fork GFN_to_ARC~~
- Day 2: Implement 3 hand-defined energy functions (rotation, reflection, color map)
- Day 3: Test energy discrimination on 10 single-transformation ARC tasks
- Day 4: Implement energy composition, test on 5 two-step tasks
- Day 5: **CHECKPOINT** — Evaluate if approach is working; decide continue/pivot
- Day 6-7: Either continue Subproblem 1 experiments OR pivot to Subproblem 2

**Week 2: Results + Writing**

- Day 8-9: Complete main experiments (50+ tasks if possible)
- Day 10: Run ablations, generate figures
- Day 11-12: Write paper (4-6 pages workshop format)
- Day 13: Internal review, revisions
- Day 14: **SUBMIT** to arXiv + workshop

## Key insight for avoiding "work with no fruits"

Schulman's most actionable principle: **maintain deliverable focus**. Every experiment should either (a) produce a result for the paper, (b) clearly fail and inform a pivot, or (c) provide debugging information for the next experiment. If you find yourself running experiments "to see what happens" without clear success criteria, stop and redefine the experiment.

Your dual constraint—wanting work that "deeply matters" for long-term research while avoiding "no fruits"—resolves through Schulman's incremental climbing toward ambitious goals. The compositional EBM→discrete reasoning direction matters for embodied AI (your target at Physical Intelligence, Figure AI, DeepMind Robotics). A workshop paper showing this bridge works is a fruit; a full NeurIPS paper showing it scales is a bigger fruit; even a well-written negative result explaining why it doesn't work advances understanding.

The Pareto principle here: **80% of your publishable insight will come from the first 20% of your experiments**—specifically, the Day 5 checkpoint determining whether compositional energies discriminate correctly. If yes, you have a paper. If no, you have valuable negative results plus time to pivot. Either way, you ship.
